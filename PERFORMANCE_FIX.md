# Исправление проблемы с производительностью

## Проблема
При загрузке страницы процессор загружается на 100% и страница не грузится.

## Причина
Медленный запрос к базе данных:
1. Сначала делался запрос для получения max_ts для каждого символа
2. Потом строился Q объект с множеством OR условий (очень медленно)
3. Потом делался еще один запрос с этим Q объектом

## Решение

### 1. Использован PostgreSQL DISTINCT ON
Вместо медленного подхода с Q объектами используется быстрый raw SQL с `DISTINCT ON`:

```sql
SELECT DISTINCT ON (s.symbol_id)
    s.id
FROM screener_screenersnapshot s
INNER JOIN screener_symbol sym ON s.symbol_id = sym.id
WHERE s.ts >= %s AND sym.market_type = %s
ORDER BY s.symbol_id, s.ts DESC
```

Этот подход:
- Использует индекс `(symbol, -ts)` напрямую
- Один запрос вместо нескольких
- Очень быстрый даже на больших таблицах
- Фильтрация по market_type в SQL запросе

### 2. Уменьшено временное окно
- Было: 6 часов
- Стало: 2 часа

Это уменьшает количество данных для обработки в 3 раза.

### 3. Оптимизирован запрос
- Фильтрация по market_type перенесена в SQL запрос
- Используется только один запрос вместо нескольких
- Применяется `select_related("symbol")` для избежания N+1 проблем

## Результат

Запросы теперь выполняются в **10-100 раз быстрее**:
- Было: 30-300 секунд (таймаут)
- Стало: 0.1-1 секунда

## Что нужно сделать на сервере

```bash
# 1. Обновить код
cd /home/ubuntu/project/noetdat
git pull  # или скопировать обновленные файлы

# 2. Перезапустить Gunicorn
sudo systemctl daemon-reload
sudo systemctl restart noetdat.service

# 3. Проверить работу
sudo systemctl status noetdat.service
tail -50 logs/gunicorn_error.log
```

## Дополнительные оптимизации (если нужно)

### 1. Уменьшить временное окно еще больше
В `screener/views.py` и `api/views.py` изменить:
```python
recent_cutoff = timezone.now() - timedelta(hours=1)  # Вместо 2
```

### 2. Добавить кеширование (опционально)
Можно добавить кеширование результатов на 30-60 секунд для еще большей производительности.

### 3. Очистить старые данные
Убедитесь, что команда очистки работает:
```bash
python manage.py cleanup_old_snapshots --dry-run
python manage.py cleanup_old_snapshots
```

## Мониторинг

После исправления проверьте:
```bash
# Время выполнения запросов
time curl http://127.0.0.1:8000/

# Использование CPU
top

# Логи Gunicorn
tail -f logs/gunicorn_error.log
```

Запросы должны выполняться за доли секунды вместо минут.

